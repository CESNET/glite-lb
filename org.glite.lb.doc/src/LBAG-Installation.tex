\section{Installation and Configuration}

\subsection{Complete list of packages}

\LB is currently distributed mainly in RPMs packages. It is available also in
binary form packed as .tar.gz. Recent attempts to multiplatform porting and
recent ETICS building system development promise a future possibility to
distribute the software in other distribution formats, e.g. DEB packages. 

In \LBold, the list of all LB packages was the following:

\begin{tabularx}{\textwidth}{>{\tt}lX}
glite-lb-common & common files \\ 
glite-lb-client & client library and CLI tools\\ 
glite-lb-client-interface & client library interface (header files) \\ 
glite-lb-logger & local-logger and inter-logger \\ 
glite-lb-proxy & proxy (restricted server used by WMS)\\ 
glite-lb-server & server \\ 
glite-lb-server-bones & multi-process server building library \\ 
glite-lb-utils & auxiliary utilities \\ 
glite-lb-ws-interface & web service interface  \\
glite-security-gsoap-plugin & GSS wrapper and GSS plugin for gSoap
\end{tabularx}

In \LBnew, the code has been restructured quite a lot, especially the dependencies were lightened,
and the new list of packages is now the following:

\begin{tabularx}{\textwidth}{>{\tt}lX}
glite-lb-doc & documentation \\ 
glite-lb-common & common files \\ 
glite-lb-client & client library and CLI tools\\ 
glite-lb-logger & local-logger and inter-logger \\
glite-lb-server & server, including merged proxy functionality \\
glite-lb-state-machine & state machine and LB plugin for Job Provenance \\ 
glite-lb-utils & auxiliary utilities \\
glite-lb-ws-interface & web service interface \\
\end{tabularx}

More detailed description together with the dependencies can be read directly from each package,
for example by issuing the command 
\begin{verbatim}
   rpm -qiR <package_name>
\end{verbatim}

Some of the LB packages may depend among others also on the following packages
that are worth to mention in this documentation:

\begin{tabularx}{\textwidth}{>{\tt}lX}
glite-jobid-api-c & gLite jobId C API library \\ 
% pro beh LB neni triba
% glite-jp-common & JP common files \\ 
glite-lbjp-common-db & database access layer \\
glite-lbjp-common-maildir & persistent request spool management \\
glite-lbjp-common-server-bones & multi-process server building library \\
glite-lbjp-common-trio & extended printf implementation \\
glite-security-gss & GSS wrapper \\
glite-security-gsoap-plugin & GSS plugin for gSoap \\
\end{tabularx}

where all \verb'glite-lbjp-common-*' packages are common both to \LB and 
Job Provenance (\JP).


\subsection{\LB server}

\subsubsection{Hardware requirements}
\label{inst:hw_req}

Hardware requirements depend on performance and storage time requirements.
Disk space used by LB server consists of database space and working space 
for backup dumps and temporary files used by exports to Job Provenance and
R-GMA tables. Necessary database space can be calculated by multiplying 
job retention length (job purge timeout), job submission rate, and  
per-job space requirements (120\,KB per job is recommended for current common 
EGEE usage pattern; jobs can consume more than that with use of very long
JDL descriptions, user tags, or very high number of resubmissions).
For temporary files, approximately 10\,GB is sufficient for LB server setups
working normally, more can be needed when backlog forms in data export
to any external service. For example, typical setup processing 40\,000 jobs per 
day where all jobs are purged after 10 days needs about 58 gigabytes
($10 \cdot 40000 \cdot 120 \mbox{KB (per job)} + 10$) not accounting for operating 
system and system logs.

For smooth handling of 40\,000 jobs/day, this or better machine configuration 
is necessary:
\begin{itemize}
\item 1GB RAM
\item CPU equivalent of single Xeon/Opteron 1.5GHz
\item single 7200 rpm SATA disk.
\end{itemize}
In order to achieve higher performace, following changes are recommended:
\begin{itemize}
\item Faster disks. Disk access speed is crucial for LB server, couple of 15k rpm
SCSI or SAS disks (one for MySQL database data file, the second for DB logs, LB server's
working directories, and operating system files) or RAID with battery backed 
write-back cache is preferable.
\item More memory. Large RAM improves performance through memory caching,
relative speed gain is likely to be rougly proportional to memory/database size ratio.
To use 3\,GB or more efficiently, 64bit OS and MySQL server versions are recommended.
\item Faster or more processors. CPU requirements scale approximately linearly with
offered load.
\end{itemize}

\subsubsection{Standard installation}

Install and configure OS and basic services (certificates, CAs, time synchronization, software repositories) according to the \htmladdnormallink{https://twiki.cern.ch/twiki/bin/view/LCG/GenericInstallGuide310}{https://twiki.cern.ch/twiki/bin/view/LCG/GenericInstallGuide310}. Then should be installed glite-LB metapackage from appropriate gLite software repository.

YAIM configuration for \emph{glite-LB} node type 
(\texttt{/opt/glite/yaim/bin/yaim -c -s site-info.def -n glite-LB}) 
can done then. Available parameters specific to LB server are:

%variable&meaning&default value &further details\\
\begin{itemize}
\item \texttt{MYSQL\_PASSWORD} -- root password of MySQL server (mandatory)
\item \texttt{GLITE\_WMS\_LCGMON\_FILE} -- pathname of file where job state
export data are written for use by lgcmon/R-GMA 
(default: \texttt{/var/glite/logging/status.log}
\item \texttt{GLITE\_LB\_EXPORT\_PURGE\_ARGS} -- purge timeouts (default: \texttt{--cleared 2d --aborted 15d --cancelled 15d --other 60d})
\item \texttt{GLITE\_LB\_EXPORT\_ENABLED} -- set to \texttt{true} for export to JP, installed glite-lb-client and glite-jp-client are needed (default: \texttt{false})
\item \texttt{GLITE\_LB\_EXPORT\_JPPS} -- Job Provenance Primary Storage where to export purged jobs, required if export to JP is enabled
\end{itemize}

In addition to those, YAIM LB module uses following parameters:
\texttt{INSTALL\_ROOT}, \texttt{GLITE\_LOCATION\_VAR}, \texttt{GLITE\_USER}, \texttt{GLITE\_USER\_HOME},\texttt{SITE\_EMAIL}.

\subsubsection{Migration from previous versions}

Although new LB server versions are backwards compatible with already 
stored data (without need for conversion) usually, it is necessary to recreate
LB database from scratch or to change its database schema at some points.
Database schema changes are not (currently) performed automatically 
during YAIM configuration since they take a lot time and temporary disk
space.  

There were two such notable upgrades:
\begin{itemize}
\item Version 1.4.3 of \textt{glite-lb-server} package. This \LB server version introduced optional use of database transactions for \LB database updates in order to improve their performace. This feature is switched on by default when underlaying MySQL database uses transactional InnoDB tables. For new installations, YAIM configuration process will create transactional database automatically. Existing databases can be converted using provided SQL command script:
\begin{quote}
\verb'mysql -u lbserver lbserver20 </opt/glite/etc/glite-lb-dbsetup-migrate2transactions.sql'
\end{quote}

\item \LB version 2.0.  This version features merged \LB server and  proxy services using single database, pointers to purged jobs (``zombies'') and other improvements requiring database schema changes. Existing databases must be converted using provided shell script \verb'/opt/glite/etc/glite-lb-migrate_db2version20'. Use \verb'-s' parameter to convert database used by \LB server previously and \verb'-p' to convert database used by \LB proxy before. Optionally, unnecesary index can be dropped (this operation is likely to take a lot of time when applied to large database):
\begin{quote}
\verb'mysql -u lbserver lbserver20 -e "alter table events drop index host"
\end{quote}
\end{itemize}

%\TODO{automated conversion through YAIM ?}

\subsubsection{Index configuration}

Initial YAIM configuration does not create any \LB indexes, see Section~\ref{maintain:index}} for instructions
on changing \LB server index configuration.


\subsubsection{Server superusers}
\label{inst:superusers}

Certain administrative operations (identified below when appropriate) on \LB
server are privileged and special authorization is required to invoke them.
Privileged user can also access job data owned by other users, bypassing the
standard \LB access control mechanisms.  By default, the \LB server identity
(X509 certificate subject name) is considered privileged.  Additional
administrator identitites can be specified in a \emph{superusers file},
specified by the \verb'--super-users-file' server option.  A client is granted
superuser privileges if they present credentials matching the superusers
specifications in the file.  The file consists of one or more lines, each one
containing either a subject name or VOMS attribute(s) in the FQAN format (in
the latter case the line must start with \verb'FQAN:').  Note that VOMS anchors
must be configured properly on the \LB machine when FQANs are used.  After
changing the file, the server has to be restarted. 

The default startup script checks for existence of 
/opt/glite/etc/LB-super-users and uses it eventually.

\subsubsection{Authorization of event originators}
\LBold allowed anyone possessing a trusted digital certificate to send an
arbitrary event to the \LB server. While enabling an easy setup, such an
arrangement does not comply with some contemporary requirements, \eg job
traceability, since the data could be distorted by a malicious user.  In order
to strengthen trustworthiness of data provided, the \LBnew server has
introduced an authorization mechanism to control the originators of events.
The authorization model presumes a set of trusted components that are only
allowed to send some types of events, while other types can be logged by any
user. Being based on the LCAS schema~\cite{lcas}, the authorization mechanism
allows to make use standard LCAS plugins, \eg to ban particular users or limit
the service to a specific virtual organization. The \LB server is shipped with
an LCAS plugin providing more fine-grained access control based on event types.

The LCAS-based authorization must be enabled in the \LB configuration.

\TODO{policy format}

% \subsubsection{Notification delivery}\TODO{co tu ma byt?}

\subsubsection{Export to R-GMA}

\LB server can export information on job state changes to R-GMA infrastructure through \verb'lcgmon' 
in real time. This export is enabled by YAIM by default and uses \verb'GLITE_WMS_LCGMON_FILE' 
environmental variable to retrieve name of log file which is to be consumed by \verb'lcgmon' (usually
\verb'/var/glite/logging/status.log'). The log file has to be rotated regularly.

\subsubsection{Data backup}
\label{inst:backup}

Data stored \LB server can be backed up using backups of underlaying database or using \verb'glite-lb-dump' utility.
The latter has some advantages, see Section~\ref{run:dump} for details.

\subsubsection{Purging old data}
\label{inst:purge}

Initial YAIM configuration creates a cron job which runs once a day and purges old 
data (jobs in Cleared state after two days, Aborted and Cancelled after 15 days, and other jobs 
after 60 days of inactivity). It is recommended to run the cron jobs more often (in order to purge less jobs
during single run) if event queue backlogs form in client WMS machines when the purging cron jobs is running.
For details on setting job purge timeouts, see Sect.~\ref{run:purge}.


\subsubsection{Exploiting parallelism}

\LB server uses 10 worker processes (threads) to handle active client accesses (inactive connections are killed
when necessary). Each worker process uses separate connection to database server. Number of worker processes 
can be changed by adding \verb'--slaves' parameter with desired number to servers command line
using \verb'GLITE_LB_SERVER_OTHER_OPTIONS' variable.

\subsubsection{Tuning database engine}
\label{inst:db_tuning}

In order to achieve high performance with LB server underlaying MySQL 
database server has to be configured reasonably well too. 
Default values of some MySQL settings are likely to be suboptimal
and need tuning, especially for larger machines.
These are MySQL configuration variables (to be configured in \texttt{[mysqld]} 
section of \texttt{/etc/my.cnf}) that need tuning most often:
\begin{itemize}
\item \texttt{innodb\_buffer\_pool\_size} -- size of database memory pool/cache. 
It is generally recommended to set it to aroung 75\% of RAM size
(32bit OS/MySQL versions limit this to approx. 2GB due to address space 
constraints).

\item \texttt{innodb\_flush\_logs\_at\_trx\_commit} -- frequency of flushing to disk.
Recommended values include:
\begin{itemize}
\item 1 (default) -- flush at each write transaction commit; relatively
slow without battery-backed disk cache but offers highest level of data safety
\item 0 -- flush once per second; fast, use if loss of latest updates on MySQL
or OS crash (e.g. unhandled power outage) is acceptable (database remains consistent)
\end{itemize}

\item \texttt{innodb\_log\_file\_size} -- size of database log file. Larger values
save some I/O activity, but also make database shutdown and crash recovery slower.
Recommended value: 50MB. Clean mysqld shutdown and deletion of log files 
(\texttt{/var/lib/mysql/ib\_logfile*} by default) is necessary before change.

\item \texttt{innodb\_data\_file\_path} -- path to main database file. File on
disk separate from OS and MySQL log files (\texttt{innodb\_log\_group\_home\_dir} variable,
\texttt{/var/lib/mysql/} by default) is recommended.

\end{itemize}

\subsection{\LB proxy}

All necessary configuration of standalone \LB proxy is done by YAIM,
previous \LB server section applies to merged server+proxy setups (\LBnew only).

\subsection{\LB logger}

All necessary configuration of normal \LB logger is done by YAIM.

\subsection{Smoke tests}

\TODO{ljocha: get something from the old testing docs}

Thorough tests of \LB, including performance measurement, are
covered in the \LB Test Plan document \cite{lbtp}.
This section describes only elementary tests that verify basic
functionality of the services.

The following test description assumes the \LB services installed
and started as described above.

\def\req{\noindent\textbf{Prerequisities:}\xspace}
\def\how{\noindent\textbf{How to run:}\xspace}
\def\result{\noindent\textbf{Expected result:}\xspace}

\subsubsection{Job registration}

Register a~new job with the \LB server and check that its status is
reported correctly.

\req Installed glite-lb-client package, valid user's X509 credentials,
known destination (address:port) of running \LB server.
Can be invoked from any machine.

\how 
\begin{quote}
\verb'/opt/glite/examples/glite-lb-job_reg -m' \emph{my.server.name:port}
\end{quote}
A~new jobid is generated and printed.  Run 
\begin{quote}
\verb'/opt/glite/examples/glite-lb-job_status' \emph{the\_\/new\_\/jobid}
\end{quote}

\result
The command should report ``Submitted'' job status.

\subsubsection{Logging events via lb-logger}

Send several \LB events, simulating normal job life cycle.
Checks delivery of the events via \LB logger.

\req Installed glite-lb-client package, valid user's X509 credentials,
known destination (address:port) of running \LB server.
Must be run on a~machine where glite-lb-logger package is set up and running.

\how
\begin{quote}
\verb'/opt/glite/examples/glite-lb-running.sh -m ' \emph{my.server.name:port}
\end{quote}

The command prints a~new jobid, followed by diagnostic messages as the events are logged. 
Check the status of the new job with
\begin{quote}
\verb'/opt/glite/examples/glite-lb-job_status' \emph{the\_\/new\_\/jobid}
\end{quote}

\result
Due to asynchronous event
delivery various job states can be reported for limited time (several seconds).
Finally the
``Running'' status should be reached.

\subsubsection{Logging events via lb-proxy}

Send events via \LB proxy. Checks the proxy functionality.

\TODO{}

\subsubsection{Notification delivery}

Register for receiving notifications, and log events which trigger
the notification delivery. Checks the whole notification mechanism.

\TODO{mozna jen odkaz do User Guide}
