%
%% Copyright (c) Members of the EGEE Collaboration. 2004-2010.
%% See http://www.eu-egee.org/partners for details on the copyright holders.
%% 
%% Licensed under the Apache License, Version 2.0 (the "License");
%% you may not use this file except in compliance with the License.
%% You may obtain a copy of the License at
%% 
%%     http://www.apache.org/licenses/LICENSE-2.0
%% 
%% Unless required by applicable law or agreed to in writing, software
%% distributed under the License is distributed on an "AS IS" BASIS,
%% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
%% See the License for the specific language governing permissions and
%% limitations under the License.
%
\section{\LB Architecture}

%historie: vyrobeno pro WMS v EDG, 1. a 2. verze (seq. èísla,
%cache a dotazy na stavy), v EGEE gLite---ustabilnìní, proxy

\LB's primary purpose is tracking WMS jobs as they are processed by
individual Grid components, not counting on the WMS to provide this data.
The information gathered from individual sources is collected, stored in
a database and made available at a single contact point. The user gets a
complete view on her job without the need to inspect several service logs
(which she may not be authorized to see in the entirety or she may not be
even aware of their existence).

While \LB keeps track of submitted and running jobs, the information is
kept by the \LB service also after the job has been finished (successfully
completed its execution, failed, or has been canceled for any reason). The 
information is usually available several days after the last event
related to the job arrived, to give user an opportunity to check the job's
final state and eventually evaluate failure reasons.

As \LB collects also information provided by the WMS, the WMS services are
no longer required to provide a job-state querying interface.  Most of the
WMS services can be even designed as stateless---they process a~job and
pass it over to another service, not keeping state information about the job
anymore.  During development and deployment of the first WMS version this
approach turned to be essential in order to scale the services to the
required extent~\cite{jgc}. 

\LB must collect information about all important events in the Grid job
life. These include transitions between components or services, results
of matching and brokerage, waiting in queue systems or start and end of
actual execution.
We decided to achieve this
goal through provision of an API (and the associated library) and
instrumenting individual WMS services and other Grid components with direct
calls to this API. But as \LB is a centralized service (there exists 
a single point where all information about a particular job must
eventually arrive), direct synchronous transfer of data could have
prohibiting impact on the WMS operation.
The temporary unavailability or overload of the remote \LB service
must not prevent (nor block) the instrumented service to perform as usual.
An asynchronous model with a clear \emph{asynchronous delivery
semantics}, see Sect.~\ref{gathering}, is used to address this issue.

As individual Grid components have only local and transient view of a
job, they are able to send only information about individual events. This
raw, fairly complex information is not 
a~suitable form to be presented to the user for frequent queries. It must
be processed at the central service and users must be presented primarily with this
processed form. This form is derived from the \emph{job state} and its
transition, not from the job events themselves. The raw information is
still available, in case more detailed insight is necessary.

While the removal of state information from (some of) the WMS services
helped to achieve the high scalability of the whole WMS, the state
information is still essential for the decisions made within the resource
broker or during the matchmaking process. 
\Eg decision on job resubmission is usually affected by the number of
previous resubmission attempts. This kind of information is currently
available in the \LB only, so the next ``natural'' requirement has been
to provide an interface for WMS (and other) services to the \LB to query
for the state information. However, this requirement contains two
contradictions: (i)~due to the asynchronous event delivery model, the \LB
information may not be up to date and remote queries may lead to unexpected
results
(or even inconsistent ones---some older information may not be available for 
one query but may arrive before a subsequent query is issued),
and (ii)~the dependence on a~remote service to provide vital state information
may block the local service if the remote one is not responding.
These problems are addressed by providing \emph{local view} of the \LB data,
see Sect.~\ref{local}




\subsection{Concepts}


\subsubsection{Jobs and events}
To keep track of user jobs on the Grid, we first need some reliable
way to identify them. This is accomplished by assigning a unique
identifier, which we call \emph{jobid} (``Grid jobid''), to every job
before it enters the Grid. A~unique jobid is assigned, making it the
primary index to unambiguously identify any Grid job. This jobid is then
passed between Grid
components together with the job description as the job flows through
the Grid; the components themselves may have (and usually do) their
own job  identifiers, which are unique only within these components.

Every Grid component dealing with the job during its lifetime 
may be a source of information about the job. The \LB gathers information
from all the 
relevant components. This information is obtained in the form of
\LB events, pieces of data generated by Grid components, which mark
important points in the job lifetime (\eg passing of job control
between the Grid components are important milestones in job lifetime
independently on the actual Grid architecture); see Appendix~\ref{a:events}
for a~complete list. We collect those
events, store them into a database and simultaneously process them to
provide higher level view on the job's state.  The \LB collects redundant
information---the event scheme has been designed to be as redundant as
possible---and this redundancy is used to improve resiliency in a
presence of component or network failures, which are omnipresent on any
Grid. 

The \LB events themselves are structured into \emph{attribute}~=
\emph{value} pairs, the set of required and optional attributes is defined by the
event \emph{type} (or scheme). For the purpose of tracking job status on
the Grid and with the knowledge of WMS Grid middleware structure we
defined an \LB schema with specific \LB event
types (see Appendix\ref{a:events}).
The schema contains a common base, the attributes that must be assigned
to every single event. The primary key is the jobid, which is also one of
the required attributes. Among other common attributes belong currently the
timestamps of the event origin and of the event arrival to LB, 
generating component name, the event type, its priority and sequence code 
(see Sect.~\ref{evprocess}) and so on. For a complete list of attributes
see \cite{lbdg}.

While the necessary and sufficient condition for a global jobid is
to be Grid-wide unique, additional desired property relates to the
transport of events through the network: All events belonging to the same
job must be sent to the same \LB database. This must be done on a~per
message basis, as each message may be generated by a different component.
The same problem is encountered
by users when they look for information about their job---they need
to know where to find the appropriate \LB database too.
While it is possible to devise a global service where each job registers
its jobid together with the address of the appropriate database, such a
service could easily become a bottleneck. We opted for another solution,
to keep the address of the \LB database within the jobid (actually,
fully qualified hostname is strongly recommended instead of numeric address
and numeric IPv6 address is not supported for backward compatibility reasons).
This way, finding appropriate \LB database address becomes a local operation
(at most parsing the jobid) and users can use the same mechanism when
connecting to the \LB database to retrieve information about a particular
job (users know its jobid). To simplify the situation even further, 
the jobid has the form of an URL, where the protocol part is
``https'', server and port identify the machine running the appropriate 
\LB server
(database) and the path contains base64 encoded MD5 hash of random
number, timestamp, PID of the generating process and IP address of the
machine, where the jobid was generated. Jobid in this form can be
used even in the web browser to obtain information about the job,
provided the \LB database runs a web server interface. This jobid is
reasonably unique---while in theory two different job identifications can
have the same MD5 hash, the probability is low enough for this jobid to
represent a globally unique job identification.

%zajímá nás job, globální id, v¹echna data vzta¾ena k~nìmu, syrové události
%
%více zdrojù dat pro jeden job, redundance, shromá¾dìní na jednom místì

\subsubsection{Event gathering}
\label{gathering}
%zdroje událostí, lokální semantika logování, store-and-forward

As described in the previous section, information about jobs are
gathered from all the Grid components processing the job in the form
of \LB events. The gathering is based on the \emph{push} model where
the components are actively producing and sending events. The push model
offers higher performance and scalability than the pull model, where the
components are to be queried by the server. In the push model, the \LB
server does not even have to know the event sources, it is sufficient
to listen for and accept events on defined interface. 

The event delivery to the destination \LB server is asynchronous and
based on the store--and--forward model to minimize the performance
impact on component processing. Only the local processing is synchronous,
the \LB event is sent synchronously only to the nearest \LB component
responsible for event delivery. This component 
is at the worst located in the same local area network (LAN) and usually
it runs on the same host as
the producing component. The event is stored there (using persistent
storage -- disk file) and confirmation is sent back to the
producing component. From the component's point of view, the
send event operation is fast and reliable, but its success only means
the event was accepted for later delivery. The \LB delivery components
then handle the event asynchronously and ensure its delivery to the
\LB server even in the presence of network failures and host reloads.

It is important to note that this transport system does not guarantee
ordered delivery of events to the \LB server; it \emph{does} guarantee
reliable and secure delivery, however. The guarantees are statistical
only, as the protocol is not resilient to permanent disk or node crashes
nor to the complete purge of the data from local disk. Being part of the
trusted infrastructure, even the local \LB components should run on
a trusted and maintained machine, where additional reliability may be
obtained \eg by a RAID disk subsystem.

\subsubsection{Event processing}%
\label{evprocess}

%diagram stavù, mapování událostí na hrany

%uspoøádání událostí -- seq. èísla, vèetnì shallow vìtví

% ! abstraktne, nemame jeste komponenty

% prichazeji udalosti, vice zdroju, zmenene poradi (az ztraty)
% redundantni informace
% motivace: usetrit uzivatele, hlasit agregovany stav jobu
As described in the previous section, \LB gathers raw events from various
Grid middleware components and aggregates them on a~single server
on a per-job basis.
The events contain a very low level detailed information about the job
processing at individual Grid components. This level of detail is
valuable for tracking various problems with the job and/or the
components, and as complementary events are gathered (\eg each job control
transfer is logged independently by two components), the information is
highly redundant. Moreover, the events could arrive in wrong order,
making the interpretation of raw information difficult and not
straightforward.
Users, on the other hand, are interested in a much higher view, the
overall state of their job. 

For these reasons the raw events undergo complex processing, yielding 
a~high level view, the \emph{job state}, that is the primary type of data
presented to the user.
Various job states form nodes of the job state diagram (Fig.~\ref{f:jobstat}).
See Appendix~\ref{a:jobstat} for a list of the individual states.

% stavovy automat
% obrazek: stavovy diagram

\begin{figure}[ht]
\centering
\includegraphics[width=.6\hsize]{images/wms2-jobstat}
\caption{\LB\ job state diagram}
\label{f:jobstat}
\end{figure}

% typ udalosti -> zmeny typu stavu
\LB\ defines a~\emph{job state machine} that is responsible for updating
the job state on receiving a~new event.
The logic of this algorithm is non-trivial; the rest of this section deals
with its main features.

Transitions between the job states happen on receiving events of particular
type coming from particular sources.
There may be more distinct events assigned to a~single edge of the state diagram.
For instance, the job becomes \emph{Scheduled} when it enters batch system
queue of a~Grid computing element.
The fact is witnessed by either \emph{Transfer/OK} event reported by 
the job submission service or by \emph{Accept} event reported by the computing
element. Receiving any one of these events (in any order) triggers the
state change.

% fault tolerance
This way, the state machine is highly fault-tolerant---it can cope with
delayed, reordered or even lost events.
For example, when a~job is in the \emph{Waiting} state and the \emph{Done}
event arrives, it is not treated as inconsistency but it is assumed that
the intermediate events are delayed or lost and the job state is switched
to the \emph{Done} state directly.

% udalosti nesou atributy, promitaji se do stavu 
The \LB events carry various common and event-type specific attributes,
\eg \emph{timestamp} (common) or \emph{destination} (\emph{Transfer} type).
The job state record contains, besides the major state identification,
similar attributes, \eg
an array of timestamps indicating when the job entered each state,
or \emph{location}---identification of the Grid component which is currently
handling the job.
Updating the job state attributes is also the task of the state machine,
employing the above mentioned fault tolerance---despite a~delayed event
cannot switch
the major job state back
it still may carry valuable information to update the job state attributes.

% typy jobu
Jobs monitored by \LB service may have different type. For gLite jobs, \LB supports
simple jobs and jobs representing \emph{set of jobs} -- \emph{DAGs} (with dependencies between 
subjobs described by direct acyclic graph) and \emph{collections} (without dependencies 
between subjobs). 
In these cases, subjobs are monitored in standard way, with one extensions - when job
status is changed, information is propagated also to the job representing corresponding 
collection or DAG.
Job representing collection or DAG can be used to monitor status of the set, including
information like how many subjobs is already finished etc.
Support for non-gLite jobs, namely for PBS or Condor systems, is described in section
\ref{sec:nonglite}


\subsubsection{Event ordering}%
\label{evorder}

As described above, the ability to correctly order arriving events is
essential for the job state computation.
As long as the job state diagram was acyclic (which was true for the
initial WMS release), each event had its unique place in the expected sequence
hence event ordering could always be done implicitly from the
context.
However, this approach is not applicable once job resubmission
yielding cycles in the job state diagram was introduced.

Event ordering that would rely on timestamps assigned to events upon
their generation, assuming strict clock synchronization over the Grid,
turned to be a~naive approach.
Clocks on real machines are not precisely synchronized and there are no reliable
ways to enforce synchronization across administrative domains.

To demonstrate a problem with desynchronized clocks, that may lead to
wrong event interpretation, let us consider a~simplified example
in Tab.~\ref{t:cefail}.
%
\iffalse %stare
% usporadani udalosti -- seq. cisla
% priklad problemu
So far we assumed that the state machine is able to detect a~delayed event.
As the state diagram contains cycles, delay cannot be detected from the type
of the event only.
The simplest approach is relying on the event timestamps.
However, in the Grid environment one cannot assume strictly synchronized clocks.
Table~\ref{t:cefail} shows a~simplified example of the problem caused by delayed
clocks. 
\fi
%
\begin{table}[ht]
\begin{center}
\begin{tabular}{rlrl}
1.&WM: Accept&
6.&WM: Accept\\
2.&WM: Match $A$&
7.&WM: Match $B$\\
3.&WM: Transfer to $A$&
8.&WM: Transfer to $B$\\
4.&CE~$A$: Accept &
9.&CE~$B$: Accept \\
5.&CE~$A$: Run &
10.&CE~$B$: Run \\
\dots & $A$ dies\\
\end{tabular}
\end{center}
\caption{Simplified \LB events in the CE failure scenario}
\label{t:cefail}
\end{table}
%
We assume that the workload manager (WM) sends the job to a~computing element
(CE)~A, where it starts running but the job dies in the middle.
The failure is detected and the job is resubmitted back to the WM which sends it to CE~B then.
However, if A's clock is ahead in time and B's clock is correct (which
means behind the A's clock), the events in the right column are treated
as delayed. The state machine will interpret events incorrectly, assuming
the job has been run on B before sending it to A.
The job would always (assuming the A's events arrive before B's events to
the \LB) be reported as ``\emph{Running} at A'' despite
the real state should follow the \emph{Waiting} \dots \emph{Running} sequence.
Even the \emph{Done} event can be sent by B with a timestamp that says
this happened before the job has been submitted to A and the job state
will end with a discrepancy---it has been reported to finish on B while
still reported to run on A.

Therefore we are looking for a~more robust and general solution. We can
discover severe clock bias if the timestamp on an event is in a future
with respect to the time on an \LB server, but this is generally a dangerous
approach (the \LB server clock could be severely behind the real time).
We decided not to rely on absolute time as reported by timestamps, but to
introduce a kind of \emph{logical time} that is associated with the logic
of event generation.
The principal idea is arranging the pass through the job state 
diagram (corresponding to a~particular job life), that may include
loops, into an execution tree that represents the job history.
Closing a~loop in the pass through the state diagram corresponds
to forking a~branch in the execution tree.
The scenario in Tab.~\ref{t:cefail} is mapped to the tree in
Fig.~\ref{f:seqtree}.
The approach is quite general---any finite pass through any state
diagram (finite directed graph) can be encoded in this way.

\begin{figure}[ht]
\centering
\includegraphics[scale=.833]{images/seqtree}
\caption{Job state sequence in the CE failure scenario, arranged into a~tree.
Solid lines form the tree, arrows show state transitions.}
\label{f:seqtree}
\end{figure}

Our goal is augmenting \LB events with sufficient information that
\begin{itemize}
 \item identifies uniquely a~branch on the execution tree, 
 \item determines the sequence of events on the branch, 
 \item orders the branches themselves, which means that it determines
   which one is more recent.  
\end{itemize}
If such information is available, the execution tree can be
reconstructed on the fly as the events arrive, and even delayed events
are sorted into the tree correctly.  An incoming event is considered
for job state computation only if it belongs to the most recent
branch.

The situation becomes even more complicated when 
the \emph{shallow resubmission} WM advanced feature is enabled.
In this mode WM may resubmit the job before being sure the previous attempt
is really unsuccessful, potentially creating multiple parallel instances
of the job.
The situation maps to several branches of the execution tree that
evolve really in parallel.
However, only one of the job instances becomes active (really running) finally;
the others are aborted.
Because the choice of active instance is done later, 
it may not correspond to the most recent execution branch.
Therefore, when an event indicating the choice of active instance arrives,
the job state must be recomputed, using the corresponding active branch
instead the most recent one.

Section~\ref{seqcode} describes the current implementation of event
ordering mechanism based on ideas presented here.

\subsubsection{Queries and notifications}\label{retrieve}

According to the GMA classification the user retrieves data from
the infrastructure in two modes, called
\emph{queries} and \emph{notifications} in~\LB.

Querying \LB is fairly straightforward---the user specifies query
conditions, connects to the querying infrastructure endpoint, and
receives the results. 
For ``single job'' queries, where jobid is known, the endpoint (the
appropriate \LB server) is inhered from the jobid.
More general queries must specify the \LB server explicitely,
and their semantics is intentionally restricted 
to ``all such jobs known here''.
We trade off generality for performance and reliability,
leaving the problem of finding the right query endpoint(s), the right
\LB servers, to higher level information and service-discovery services.

If the user is interested in one or more jobs, frequent polling of the
\LB server may be cumbersome for the user and creates unnecessary overload
on the sever. A notification subscription is therefore available,
allowing users to subscribe to receive notification whenever a~job 
starts matching user specified conditions.
Every subscription contains also the location of the user's
listener; 
successful subscription returns time-limited \emph{notification handle}.
During the validity period of the subscription, the \LB infrastructure
is responsible for queuing and reliable delivery of the notifications.
The user may even re-subscribe (providing the original handle) with different
listener location (\eg moving from office to home), and \LB re-routes
the notifications generated in the meantime to the new destination.
The \LB event delivery infrastructure is reused for the notification
transport. Alongside it, there is a possibility to deliver notification
messages thourgh the messaging infratstructure.

\subsubsection{Local views}\label{local}
% motivace proxy

%As outlined in Sect.~\ref{reqs} 
WMS components are, besides logging
information into \LB, interested in querying this information back in order
to avoid the need of keeping per-job state information.
However, despite the required information is present in \LB,
the standard mode of \LB operation is not suitable for this purpose due
to the following reasons:
\begin{itemize}
\item Query interface is provided on the \LB component which gathers
events belonging to the same job but coming from different sources. 
Typically, this is a~remote service with respect to the event sources (WMS components).
Therefore the query operation is sensitive to any network failure that may
occur, blocking the operation of the querying service for indefinite time.
\item Due to the asynchronous logging semantics, there is a~non-zero time
window between successful completion of the logging call and the point in
time when the logged event starts affecting the query result.
This semantics may yield unexpected, seemingly inconsistent outcome. 
\end{itemize}

The problem can be overcome by introducing \emph{local view} on job data.
Besides forwarding events to
the server where events belonging to a~job are gathered from multiple sources,
\LB infrastructure can store the logged events temporarily
on the event source, and perform the processing described
in Sect.~\ref{evprocess}
In this setup, the logging vs.\ query semantics can be synchronous---it is
guaranteed that a~successfully logged event is reflected in the result of
an immediately following query,
because no network operations are involved.
Only events coming from this particular physical node (but potentially
from all services running there) are considered, thus the locality of the view. 
On the other hand, certain \LB events are designed to contain redundant
information, therefore the local view on processed data (job state) 
becomes virtually complete on a~reasonably rich \LB data source like
the Resource Broker node.


\subsection{Current \LB implementation}
The principal components of the \LB service and their interactions
are shown in Figures~\ref{f:comp-gather} (gathering and transferring
\LB events) and~\ref{f:comp-query} (\LB query and notification services).

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{images/LB-components-gather}
\caption{\LB components involved in gathering and transferring the events}
\label{f:comp-gather}
\end{figure}

\input components

\subsubsection{Sequence codes for event ordering}%
\label{seqcode}

As discussed in Sect.~\ref{evorder}, sequence codes are used as logical
timestamps to ensure proper event ordering on the \LB server. The sequence code
counter is incremented whenever an event is logged and the sequence code must
be passed between individual Grid components together with the job control.
However, a single valued counter is not sufficient to support detection of
branch forks within the execution tree. When considering again the Computing
Element failure scenario described in Sect.~\ref{evorder}, there is no way to
know that the counter value of the last event logged by the failed CE A is 5
(see Table~\ref{t:cefail} on page~\pageref{t:cefail}). 

Therefore we define a~hierarchical \emph{sequence code}---an array of
counters, each corresponding to a~single Grid component class handling the job%
\footnote{Currently the following gLite components: Network Server, Workload
Manager, Job Controller, Log Monitor, Job Wrapper, and the application itself.}.
Table~\ref{t:cefail2} below shows the same scenario with a~simplified two-counter
sequence code. The counters correspond to the WM and CE component classes
and they are incremented when each of the components logs an event.
When WM receives the job back for resubmission, 
the CE counter becomes irrelevant (as the job control is on WM now),
and the WM counter is incremented again.

\begin{table}[ht]
\begin{center}
\begin{tabular}{rlrl}
1:x&WM: Accept&
4:x&WM: Accept\\
2:x&WM: Match $A$&
5:x&WM: Match $B$\\
3:x&WM: Transfer to $A$&
6:x&WM: Transfer to $B$\\
3:1&CE~$A$: Accept &
6:1&CE~$B$: Accept \\
3:2&CE~$A$: Run &
6:2&CE~$B$: Run \\
\dots & $A$ dies\\
\end{tabular}
\end{center}
\caption{The same CE failure scenario: hierarchical sequence codes.
``x'' denotes an undefined and unused value.}
\label{t:cefail2}
\end{table}

%Events on a~branch are ordered following the lexicographical order
%of the sequence codes.
%Branches are identified according to the WM counter as WM is 
%currently the only component where branching can occur.

The state machine keeps the current (highest seen) code for the job, 
being able to detect a~delayed event by simple lexicographic comparison
of the sequence codes.
Delayed events are not used for major state computation, then.
Using another two assumptions (that are true for the current
implementation):
\begin{itemize}
 \item events coming from a~single component arrive in order,
 \item the only branching point is WM,
\end{itemize}
it is safe to qualify events with lower WM counter (than the already
received one) to belong to inactive
branches, hence ignore them even for update of job state attributes.

\subsubsection{\LB data protection}
Events passed between the \LB components as well as the results of their
processing provide detailed information about the corresponding job and its
life and users obviously expect the job data provided by the \LB server to
be credible, reflecting the real jobs' operation on the Grid. Therefore, the
data must be based solely on authentic information generated by legitimate
grid components. The job data also provides information about user's
activities, which many users want to keep private. In order to provide a
sufficient level of security, the \LB infrastructure implements a security
mechanism that provides data protection and access control to the data.

All the \LB components communicate solely over secure channels whenever they
send data over a network. The TLS protocol~\cite{tls} is used for both mutual
authentication of the client and server and also encryption of the
communication. All the \LB components as well as the clients must possess a
digital certificate that they use to prove their identity. The \LB
infrastructure supports both standard X.509 certificates or proxy
certificates~\cite{proxycert} that are standard authentication mechanism in
the gLite environment. Depending on the server configuration and action
requested, the users may be required to present VOMS attributes in their
proxy certificates.

By default, access to information about a job is only allowed to the user
who submitted the job (\ie the job owner). The job owner can also assign an
access control list to his or job in the \LB specifying other users who are
allowed to read the data from \LB. The ACLs are represented in
the GridSite GACL format~\cite{gacl2} and are stored in the \LB database
along with the job information. The stored ACL are checked on each query
requesting the data. The ACLs are under control of the job owner, who can
add and remove entries in the ACL arbitrarily using the \LB API or
command-line tools (see~\ref{e:change-acl}). Each entry of an ACL can
specify either a user subject name, a name of a VOMS group, or an attribute
specified in the Full qualified attribute name format (the FQAN support is
available since \LBver{2.0}). An ACL assigned to a job is returned as part of 
job status information.

Besides of using the ACLs, the \LB administrator can also specify a~set of
privileged users with access to all job records on a particular \LB server
(\emph{super-users}). These privileged users can \eg collect information on
usage and produce monitoring data based on the \LB information.

%Data trustworthiness - the events aren't signed, no real non-reputability or
%traceability of the event sources.


\subsection{User interaction}
\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{images/LB-components-query}
\caption{\LB queries and notifications}
\label{f:comp-query}
\end{figure}

So far we focused on the \LB internals and the interaction between its
components.
In this section we describe the interaction of users with the service.

\subsubsection{Event submission}
%implicitní -- registrace jobu, systémové události na middlewarových komponentách
The event submission is mostly implicit,
\ie it is done transparently by the Grid middleware components
on behalf of the user.
Typically, whenever an important point in the job life is reached,
the involved middleware component logs an appropriate \LB event.
This process is not directly visible to the user.

A specific case is the initial registration of the job.
This must be done synchronously, as otherwise subsequent events logged for
the same job may be refused with a ``no such job'' error report. 
Therefore submission of a job to the WMS is the only synchronous event
logging that does not return until the job is successfully registered with
the \LB server.
Moreover, the initial registration is sent by the \LB client library in
parallel to \LB proxy (Sect.~\ref{s:proxy}) and \LB server. 

On the other hand, even the registration event may carry large data
(e.g. JDL of huge job collection). Therefore 
also an additional asychronous variant is used, causing two job registration
events appear typically.

% explicitní -- user tagy, ACL
However, the user may also store information into the \LB explicitly
by logging user events---\emph{tags} (or annotations) of the
form ``name = value''.
Authorization information is also manipulated in this way. 

Description of tools for event submission and ACL manipulation 
can be found in Section \ref{s:lb-tools}


\subsubsection{Querying information}
%\TODO{prejmenovat, aby v nazvy byly Queries}

From the user point of view, the information retrieval (\emph{query}) is the
most important interaction with the \LB service.

%dotazy na stav
The typical \LB usage are queries on the high-level job state information.
\LB supports not only single job queries, it is also possible to
retrieve information about jobs matching a specific condition.
The conditions may refer to both the \LB system attributes
and the user annotations. Rather complex query semantics can be
supported, \eg 
\emph{Which of my jobs annotated as ``apple'' or ``pear'' are already
scheduled for execution and are heading to the ``garden'' computing element?}
The \LB Developer's
Guide\cite{lbdg} provides a~series of similar examples of
complex queries.

%dotazy na události
As another option, the user may retrieve raw \LB events.
Such queries are mostly used for debugging, identification of repeating
problems, and similar purposes.
The query construction refers to event attributes rather
than job state.

The query language supports common comparison operators, and it allows
two-level nesting of conditions (logically \emph{and}-ed and \emph{or}-ed).
Our experience shows that it is sufficiently strong to cover most user
requirements while being simple enough to keep the query cost reasonable.
Complete reference of the query language can be found in~\LB Developer's Guide
\cite{lbdg}.

\subsubsection{Notifications}
\LB notifications are the other mode of user interaction.

The \LB infrastructure can notify its users when something interesting happens on an \LB server
(typically a job status change). This allows users to wait comfortably until they are informed by the server, rather than having to poll the \LB server periodically to detect changes.

Users register for notifications via the notification client \verb'glite-lb-notify', described in Section
\ref{s:lb-notify} 
Conditions under which the notifications are sent can be specified. For example -- job XY reaches state
DONE. In \LBver{1.x}, one or more JOBID's are required in the condition and only a single occurence 
of a specific attribute is allowed among ANDed conditions. More complex conditions are allowed since \LBver{2.0}, including specification of job owner, or requesting to receive notifications only on actual job state change. \LBver{3.0} introduces an option to deliver notification messages over OpenWire or STOMP-based messaging infrastructure.\footnote{Such as Apache's ActiveMQ.}

Each registration is delivered to the \LB server where it is stored.
At the same time, the server generates a unique notification ID for the registration and returns it to the
user.

The registration exists only for a limited amount of time. Validity information is returned by \LB server together with
the notification ID when registering. During this period the user can attach to the server and receive notification messages,
change conditions which triger the notification, prolong validity of the registration, or remove the registration
from the \LB server. 

While the registration is valid, the user is able to repeatably connect to the \LB server from different places in the
network and continue receiving notifications associated with the given notification ID.\footnote{Should the user have opted to receive notification messages over the messaging infrastructure, then---obviously---they need to connect to the correct topic on a messaging broker, rather than contacting the \LB sever. If unsure what messaging brokers are available in your grid environment, read that information from BDII or use the \LB Server's configuration page (Section \ref{s:findbroker}).
} Notifications generated
while the user was not connected are stored and waiting until the user reconnects. 

Since \LBver{3.2}, notification messages may also contain a history of all events received for a given job so far.

\subsubsection{Caveats}
\LB is designed to perform well in the unreliable distributed Grid
environment.
An unwelcome but inevitable consequence of this design 
are certain contra-intuitive features in the system behavior,
namely:
\begin{itemize}
\item 
Asynchronous, possibly delayed event delivery may yield seemingly
inconsistent view on the job state \wrt\ information that is available
to the user via different channels.
\Eg the user may know that her job terminated because of monitoring the
application progress directly, but the \LB \emph{Done} events indicating
job termination are delayed so that \LB reports the job to be still
in the \emph{Running} state.

\item 
% sekvenèní èísla -- ¹patnì øazené události jsou ignorovány pro výpoèet stavu
Due to the reasons described in Sect.~\ref{evorder} \LB is rather sensitive
to event ordering based on sequence codes.
The situation becomes particularly complicated when there are multiple
branches of job execution.
Consequently the user may see an \LB event that is easily interpreted that
it should switch the job state, however, it has no effect in fact
because of being (correctly) sorted to an already inactive branch.

\item 
% purge -- data z~\LB\ zmizí
\LB is not a~permanent job data storage. The data get purged from the
server on timeout, unrelated to any user's action.
Therefore, the \LB query may return ``Identifier removed'' error message (or not
include the job in a list of retrieved jobs) even if the same previous
\LB query behaved differently.

\end{itemize}

\ludek{\subsection{Performance and scalability}
The \LB service was designed with performance and
scalability issues in mind. We have developed a series of tests of the
individual \LB components to measure the actual behavior under
stress conditions. These tests give us a good performance estimate of
the \LB service and help us identify and remove possible bottlenecks.

The testing itself is done by feeding the \LB components with events
prepared beforehand, using whatever protocol appropriate for the given
component. The feeding program uses a set of predefined events of a
typical job which we have chosen from the production \LB server
database. Timestamp is taken before the first event is sent, then the
feeding program begins sending events of individual jobs (the jobs are
all the same, the only difference is the jobid; the number of jobs used is
configurable). The tested component is instrumented in the source code
to break normal event processing at selected points (\eg discard
events immediately after being read to measure the input channel
performance, or discard events instead of sending them to the next
component, etc.). This segmentation of event processing enables to
identify places in the code which may slow down the event transfer. 
Optionally the events may be discarded by the next component in the
logical path. The last event of the last job is special termination
event, which is recognized when being discarded; then the second
timestamp is taken and the difference between the two gives us total
time necessary to pass given number of jobs through. 

Note that due to the asynchronous nature of the \LB service measuring for
example the time it takes to send given number of jobs does not give
us the required result, thus event (or job) throughput---when the
producer receives acknowledgment about successful send operation, it
is not guaranteed that the event passed through the component. 

The results shown in table~\ref{perf:results} give the overall
throughput components (events are discarded by the next component
on the path), with the exception of proxy, where the throughput to the
database is measured. It can be seen that the majority of code is fast
enough to handle even high event rates and that most components are up
to our goal to handle one million of jobs per day.  The first line
indicates how fast we are able to ``generate'' events in the feeding
program. 

\begin{table}[ht]
\begin{tabular}{l|r}
{\bf Component} & {\bf Job throughput (jobs/day)} \\
\hline
Test producer & 153,600,000 \\
Locallogger & 101,700 \\
Interlogger & 5,335,100 \\
Proxy & 1,267,110 \\
\end{tabular}
\caption{Performance testing results}
\label{perf:results}
\end{table}

During the performance testing we have identified two possible
bottlenecks:
\begin{itemize}
\item Opening connections and establishing SSL sessions is very
expensive operation. It hinders mainly the performance of locallogger,
because the current implementation uses one SSL session for
event.
\item Database operations. Storing events into database is expensive,
but inevitable; however we were able to optimize for example the
code checking for duplicated events.
\end{itemize}

In the current work we are addressing the issue of SSL operations by
introducing concept of SSL connection pools, which enables components
to reuse existing connections transparently without need to tear-down
and setup new SSL contexts. }

\subsection{Advanced use}

The usability of the \LB service is not limited to the simple tasks
described earlier. It can be easily extended to support real-time job
monitoring (not only the notifications) and the aggregate information
collected in the \LB servers is a valuable source of data used for post-mortem
statistical analysis of jobs and also the Grid infrastructure behavior.
Moreover, \LB data can be used to improve scheduling decisions.

\subsubsection{\LB and real time monitoring}
The \LB server is extended to provide quickly and without any substantial
load on the database engine the following data:
\begin{enumerate}
\item number of jobs in the system grouped by internal status
(\emph{Submitted}, \emph{Running}, \emph{Done},~\ldots),
\item number of jobs that reached final state in the last
hour,
\item associated statistics like average, maximum, and minimum time spent
by jobs in the system,
\item number of jobs that entered the WMS system in the last hour.
\end{enumerate}
\LB server can be regularly queried to provide this data to give an
overview about both jobs running on the Grid and also the behavior of the
Grid infrastructure as seen from the job (or end user) perspective.
Thus \LB\ becomes a~data source for various real-tim Grid monitoring tools.

\subsubsection{R-GMA feed}
\emph{Note: This feature is now obsolete and only available in \LBver{1.x}.}

The \LB server also supports streaming the most important data---the job
state changes---to another monitoring system. It works as the
notification service, sending information about job state changes to
a~specific listener that is the interface to a monitoring interface.
As a~particular example of such a generic service, the R-GMA feed component
has been developed. It supports sending job state changes to
the R-GMA infrastructure that is part of the Grid monitoring
infrastructure used in the EGEE Grid.

Only basic information about job state changes is provided
this way, taking into account the security limitation of the R-GMA. 

\subsubsection{\LB Job Statistics}
Data collected within the \LB servers are regularly purged, complicating
thus any long term post-mortem statistical analysis. Without a Job
Provenance, the data from the \LB must be copied in a controlled way and
made available in an environment where even non-indexed queries can be
asked. 

Using the \LB Job Statistics tools, one dump file per job is created 
when the job reaches a~terminal state. These dump files can be
further processed to provide and XML encoded Job History Record%
\footnote{\url{http://egee.cesnet.cz/en/Schema/LB/JobRecord}} that
contains all the relevant information from the job life. The Job History
Records are fed into a statistical tools to reveal interesting information
about the job behavior within the Grid.

This functionality is being replaced by the direct download of all the
relevant data from the Job Provenance.

\subsubsection{Computing Element reputability rank}
\label{s:ce-rank}
Production operation of the EGEE middleware showed
that misbehaving computing elements may have significant impact on
the overall Grid performance.
The most serious problem is the ``black hole'' effect---a~CE that
accepts jobs at a~high rate but they all fail there.
Such CE usually appears to be free in Grid information services
so the resource brokers keep to assign further jobs to it.

\LB data contain sufficient information to identify similar problems.
By processing the incoming data the information
was made available as on-line auxiliary statistics like
rate of incoming jobs per CE, rate of job failure, average duration of job etc.
The implementation is lightweight, allowing very high query rate.
On the RB the statistics are available as ClassAd
functions, allowing the user to specify that similarly misbehaving
CE's should be penalized or completely avoided
when RB decides where jobs get submitted.


\subsubsection{CREAM jobs}
\LBver{3.2} implements support for CREAM jobs.\footnote{Essential support was already available in \LBver{2.1}.} \LB gathers events from CREAM, performing both the mapping of CREAM attributes to WMS attributes (if possible) and storing CREAM-specific attributes. Thus, jobs submitted directly to computing elements can be tracked by \LB. Moreover, jobs submitted to WMS and from there to CREAM log WMS- and CREAM-specific data, which are integrated by \LB to provide more detailed and up-to-date job status information. CREAM job states are mapped to \LB's state diagram according to Appendix \ref{a:creammapping}.

CREAM events are generated by the CREAM executor and LRMS; the generic \emph{CREAMStatus} event is generated when CREAM notifies that the job status has been changed.

\subsubsection{Sandbox transfers}
A sandbox transfer is special entity in \LB, which tracks the transfer of input or output sandboxes of the (compute) job. Thus, it allows to check state of sandboxes giving more detailed overview of the status of compute job.

The sandbox transfer has its unique JobID and is cross-linked  with the related compute job. There are two types of sandboxes -- the input sandbox and the output sandbox. Each job has at most one input and one output sandbox associated, however, the sandbox can be created as a collection of sandboxes. The sandbox collection includes the owner, the overall status and the list of children sandboxes, whereas the regular sandbox tracks particular sandbox including data such as status, owner, source, destination. 

Having its unique JobID, sandbox transfers are  tracked by standard tools (API, command line utilities, HTTPs, notifications) in the same manner as traditional compute jobs.

\subsubsection{Non-gLite event sources}
\label{sec:nonglite}

\LB has been enhanced to support also non-gLite events, namely events from PBS
or Condor batch systems \cite{hpdc07}. These events are handeled differently from gLite events,
for a complete list of the PBS and Condor events see Appendix \ref{a:events}. 
Since job states in the batch system slightly differ from the states of a job
defined in \LB (see also Appendix \ref{a:jobstat}), events are processed separately
from gLite events. Both PBS and Condor events has its own state machine that processes the
events and determines the now state of the job.

Recently, there were also attempts to use \LB system to transport different types of events: 
Certificate Revocation Lists or syslog messages. For a detailed description see 
\cite{LB4CRL}.

\subsubsection{Controlling access to job information}
\label{sec:job-authz}
Access to information about a job subjects to strong access control
mechanism. By default, only the job owner is allowed to access the
information about their jobs. There are, however, means how additional
rights can be granted to other persons. The \LB server administrator can
specify a server-level policy, which grants specific rights to all jobs
stored in the server, please refer to the \LB Administrators' guide for
more information.

Besides the server-wide configuration, a job owner can also grant access to
their jobs to other users. Each job can be assigned an Access control list
(ACL), which specifyies which users are allowed to work with the job
information. If a job owner wants other users to be allowed to obtain
information about their job, they provide an ACL rule granting \verb'READ'
access for the users. The users can be identified either by their X.509
subject name or their VOMS attributes. As of \LBver{3.0}, it is also
possible to grant a \verb'TAG' right, which allows multiple users to add
user tags to the same job. The current ACL of a job is returned as a part
of the job status. Management of the ACL entries is done using logging a
special \LB event in a standard way (see \ref{e:change-acl} for particular
examples).

Starting from \LBver{3.0}, it is also possible to specify the owner of the
actual payload of a job. The feature has been introduced for better support
of multi-user pilot jobs, where the pilot submitter differs from the owner
of actual payload. In order to set the payload owner, the job owner has to
log a \verb'GrantPayloadOwnership' event which specifies the subject owner of the
payload owner. This event is supposed to come from a pilot job factory
which monitors the pilot jobs and keeps an overview about when a particular
payload is started. The new payload owner has to confirm the transition
with a \verb'TakePayloadOwnership' logged to the job using his or her
credentials. The payload and job owners have the same rights to the jobs, 
they both can query the job, etc. The identity of the current payload owner
is returned as part of the job status information.
